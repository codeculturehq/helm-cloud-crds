apiVersion: ceph.rook.io/v1
kind: CephCluster
metadata:
  name: {{ .Chart.Name }}
  namespace: {{ .Release.Namespace }}
spec:
  dataDirHostPath: /var/lib/rook
  {{- if .Values.monitoring.enabled }}
  mon:
    count: {{ .Values.storage.nodes }}
    allowMultiplePerNode: false
    # A volume claim template can be specified in which case new monitors (and
    # monitors created during fail over) will construct a PVC based on the
    # template for the monitor's primary storage. Changes to the template do not
    # affect existing monitors. Log data is stored on the HostPath under
    # dataDirHostPath. If no storage requirement is specified, a default storage
    # size appropriate for monitor data will be used.
    volumeClaimTemplate:
      spec:
        storageClassName: {{ .Values.storage.volumes.class }}
        resources:
          requests:
            storage: {{ .Values.monitoring.storage }}
  {{- end}}
  cephVersion:
    image: ceph/ceph:{{ .Values.image.tag}}
    allowUnsupported: false
  skipUpgradeChecks: false
  continueUpgradeAfterChecksEvenIfNotHealthy: false
  mgr:
    modules:
      - name: pg_autoscaler
        enabled: true
  dashboard:
    enabled: {{.Values.dashboard.enabled}}
    ssl: {{.Values.dashboard.ssl}}
  crashCollector:
    disable: {{.Values.crashCollector.enabled}}
  storage:
    storageClassDeviceSets:
      - name: aws-pvc
        # The number of OSDs to create from this device set
        count: {{ .Values.storage.nodes }}
        # IMPORTANT: If volumes specified by the storageClassName are not portable across nodes
        # this needs to be set to false. For example, if using the local storage provisioner
        # this should be false.
        portable: true
        # Certain storage class in the Cloud are slow
        # Rook can configure the OSD running on PVC to accommodate that by tuning some of the Ceph internal
        # Currently, "gp2" has been identified as such
        tuneSlowDeviceClass: true
        # Since the OSDs could end up on any node, an effort needs to be made to spread the OSDs
        # across nodes as much as possible. Unfortunately the pod anti-affinity breaks down
        # as soon as you have more than one OSD per node. If you have more OSDs than nodes, K8s may
        # choose to schedule many of them on the same node. What we need is the Pod Topology
        # Spread Constraints.
        # Another approach for a small number of OSDs is to create a separate device set for each
        # zone (or other set of nodes with a common label) so that the OSDs will end up on different
        # nodes. This would require adding nodeAffinity to the placement here.
        placement:
        {{ .Values.storage.placement | toYaml | nindent 10}}
        volumeClaimTemplates:
          - metadata:
              name: data
            spec:
              resources:
                requests:
                  storage: {{.Values.storage.volumes.size}}
              # IMPORTANT: Change the storage class depending on your environment (e.g. local-storage, gp2)
              storageClassName:  {{ .Values.storage.volumes.class }}
              volumeMode: Block
              accessModes:
                - ReadWriteOnce
  disruptionManagement:
    managePodBudgets: false
    osdMaintenanceTimeout: 30
    manageMachineDisruptionBudgets: false
    machineDisruptionBudgetNamespace: openshift-machine-api